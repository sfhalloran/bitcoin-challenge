* Does this data set even make sense? What are the limitations of this data set?
Answer: The data set makes sense, but volume_traded and trades_count tell us very little new information but add complexity (and require use of MinMaxScaler), so I excluded them from features. The main limitations of this data set are the lack of fluctuation in prices relative to the bitcoin price and the shortness of the time period it captures. If the model trains on this data, it will do poorly predicting and/or retraining on a day with large and/or rapid changes to prices (which is an answer to the last non-bolded question). I would have liked a bigger data set (as in more seconds, not shorter intervals) with a higher standard deviation of prices.
* Is the lookback window of 60 seconds helpful? What are its limitations? What other features would you want to see in this data set?
Answer: The lookback window of 60 seconds is helpful because most 60-second windows in this dataset are decently representative of its overall behavior. You can definitely afford to make the window a little bigger without risk of overfitting — a window of 240 seconds (4 minutes) worked well for me. I would have liked to see the price_high-price_low and price_open-price_closed differences for each day, but I suppose I could have made those columns myself (and I would have had to standardize the data). The features of the data set were sufficient for prediction.
* If you used a different model, why'd you choose this model? What about it made it work for this problem? Is this model complex and if so, is the complexity necessary? Is it intuitive enough to explain it to a lay-person? What was your optimizing metric? What were the hyperparameters and why'd you choose them?
Answer: A neural network is overly complex for a dataset of this relatively small size and uniform character. Ridge was my choice of linear model for its lack of complexity (it’s just OLS with a penalty!) but got slightly better results with Bayesian Ridge, since it is a little more flexible than Ridge, which uses L2 norm regularization. I was able to explain Ridge to my local lay-person, but not the Bayesian method. My optimizing metric was RMSE, which is highly intuitive  — my RMSE was 0.492, so our price_high guesses were generally off by that much. I used the default hyperparameters because I saw almost no change when playing around with weights and noise, and 300 max iterations was more than sufficient.
* If we see data for more than a single day's worth of prices, how do expect the model to perform? Will it generalize well to new data? Will retraining with this new data be an issue for this model?
Answer: See the first question
